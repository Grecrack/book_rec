{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimgr\\AppData\\Local\\Temp\\ipykernel_17572\\3633506929.py:9: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv('Data/BX_book/BX-Books.csv', sep=';', encoding=\"latin-1\", on_bad_lines='skip')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Training data contains 824908 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=1`. Either provide more data, or a different value for the `validation_split` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39mAdam())\n\u001b[0;32m     54\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[train\u001b[39m.\u001b[39;49muser\u001b[39m.\u001b[39;49mvalues, train\u001b[39m.\u001b[39;49mbook\u001b[39m.\u001b[39;49mvalues], y\u001b[39m=\u001b[39;49mtrain\u001b[39m.\u001b[39;49mbookRating\u001b[39m.\u001b[39;49mvalues, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     57\u001b[0m \u001b[39m# Evaluate model\u001b[39;00m\n\u001b[0;32m     58\u001b[0m mse \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate([test\u001b[39m.\u001b[39muser\u001b[39m.\u001b[39mvalues, test\u001b[39m.\u001b[39mbook\u001b[39m.\u001b[39mvalues], test\u001b[39m.\u001b[39mbookRating\u001b[39m.\u001b[39mvalues)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\engine\\data_adapter.py:1687\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1684\u001b[0m split_at \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(math\u001b[39m.\u001b[39mfloor(batch_dim \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m validation_split)))\n\u001b[0;32m   1686\u001b[0m \u001b[39mif\u001b[39;00m split_at \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m split_at \u001b[39m==\u001b[39m batch_dim:\n\u001b[1;32m-> 1687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1688\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTraining data contains \u001b[39m\u001b[39m{batch_dim}\u001b[39;00m\u001b[39m samples, which is not \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1689\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msufficient to split it into a validation and training set as \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1690\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mspecified by `validation_split=\u001b[39m\u001b[39m{validation_split}\u001b[39;00m\u001b[39m`. Either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1691\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprovide more data, or a different value for the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1692\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`validation_split` argument.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1693\u001b[0m             batch_dim\u001b[39m=\u001b[39mbatch_dim, validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[0;32m   1694\u001b[0m         )\n\u001b[0;32m   1695\u001b[0m     )\n\u001b[0;32m   1697\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split\u001b[39m(t, start, end):\n\u001b[0;32m   1698\u001b[0m     \u001b[39mif\u001b[39;00m t \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Training data contains 824908 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=1`. Either provide more data, or a different value for the `validation_split` argument."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# Load Data\n",
    "books = pd.read_csv('Data/BX_book/BX-Books.csv', sep=';', encoding=\"latin-1\", on_bad_lines='skip')\n",
    "books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']\n",
    "users = pd.read_csv('Data/BX_book/BX-Users.csv', sep=';', on_bad_lines='skip', encoding=\"latin-1\")\n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "ratings = pd.read_csv('Data/BX_book/BX-Book-Ratings.csv', sep=';', on_bad_lines='skip', encoding=\"latin-1\")\n",
    "ratings.columns = ['userID', 'ISBN', 'bookRating']\n",
    "\n",
    "# Merge books and ratings\n",
    "data = pd.merge(ratings, books, on='ISBN')\n",
    "\n",
    "# Create user and book id mapping\n",
    "user_ids = data['userID'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "user_encoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "book_ids = data['ISBN'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}\n",
    "\n",
    "# Map user and book ids to user and book indices\n",
    "data['user'] = data['userID'].map(user2user_encoded)\n",
    "data['book'] = data['ISBN'].map(book2book_encoded)\n",
    "\n",
    "# Split data into training and testing set\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get number of users and books\n",
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)\n",
    "\n",
    "# Set embedding dimension\n",
    "embedding_size=10\n",
    "\n",
    "# Build model\n",
    "user_input = Input(shape=[1])\n",
    "user_embedding = Embedding(num_users, embedding_size)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "book_input = Input(shape=[1])\n",
    "book_embedding = Embedding(num_books, embedding_size)(book_input)\n",
    "book_vec = Flatten()(book_embedding)\n",
    "\n",
    "product = Dot(axes=1)([book_vec, user_vec])\n",
    "model = Model(inputs=[user_input, book_input], outputs=product)\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "\n",
    "# Train model\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.bookRating.values, batch_size=64, epochs=5, verbose=1, validation_split=1)\n",
    "\n",
    "# Evaluate model\n",
    "mse = model.evaluate([test.user.values, test.book.values], test.bookRating.values)\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dimgr\\AppData\\Local\\Temp\\ipykernel_17572\\87431705.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books = pd.read_csv('Data/BX_book/BX-Books.csv', sep=';', encoding=\"latin-1\", on_bad_lines='skip')\n"
     ]
    }
   ],
   "source": [
    "books = pd.read_csv('Data/BX_book/BX-Books.csv', sep=';', encoding=\"latin-1\", on_bad_lines='skip')\n",
    "books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']\n",
    "users = pd.read_csv('Data/BX_book/BX-Users.csv', sep=';', on_bad_lines='skip', encoding=\"latin-1\")\n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "ratings = pd.read_csv('Data/BX_book/BX-Book-Ratings.csv', sep=';', on_bad_lines='skip', encoding=\"latin-1\")\n",
    "ratings.columns = ['userID', 'ISBN', 'bookRating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(ratings, books, on='ISBN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = data['userID'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "user_encoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "book_ids = data['ISBN'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'] = data['userID'].map(user2user_encoded)\n",
    "data['book'] = data['ISBN'].map(book2book_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=[1])\n",
    "user_embedding = Embedding(num_users, embedding_size)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "book_input = Input(shape=[1])\n",
    "book_embedding = Embedding(num_books, embedding_size)(book_input)\n",
    "book_vec = Flatten()(book_embedding)\n",
    "\n",
    "product = Dot(axes=1)([book_vec, user_vec])\n",
    "model = Model(inputs=[user_input, book_input], outputs=product)\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12890/12890 [==============================] - 186s 14ms/step - loss: 19.4291 - val_loss: 36.0191\n",
      "Epoch 2/200\n",
      "12890/12890 [==============================] - 230s 18ms/step - loss: 19.0363 - val_loss: 35.7929\n",
      "Epoch 3/200\n",
      "12890/12890 [==============================] - 223s 17ms/step - loss: 18.6958 - val_loss: 35.6269\n",
      "Epoch 4/200\n",
      "12890/12890 [==============================] - 219s 17ms/step - loss: 18.3788 - val_loss: 35.4794\n",
      "Epoch 5/200\n",
      "12890/12890 [==============================] - 215s 17ms/step - loss: 18.0959 - val_loss: 35.3539\n",
      "Epoch 6/200\n",
      "12890/12890 [==============================] - 212s 16ms/step - loss: 17.8166 - val_loss: 35.2549\n",
      "Epoch 7/200\n",
      "12890/12890 [==============================] - 216s 17ms/step - loss: 17.5692 - val_loss: 35.1819\n",
      "Epoch 8/200\n",
      "12890/12890 [==============================] - 219s 17ms/step - loss: 17.3357 - val_loss: 35.1201\n",
      "Epoch 9/200\n",
      "12890/12890 [==============================] - 215s 17ms/step - loss: 17.1181 - val_loss: 35.0819\n",
      "Epoch 10/200\n",
      "12890/12890 [==============================] - 212s 16ms/step - loss: 16.9108 - val_loss: 35.0510\n",
      "Epoch 11/200\n",
      "12890/12890 [==============================] - 214s 17ms/step - loss: 16.7143 - val_loss: 35.0217\n",
      "Epoch 12/200\n",
      "12890/12890 [==============================] - 209s 16ms/step - loss: 16.5298 - val_loss: 35.0093\n",
      "Epoch 13/200\n",
      "12890/12890 [==============================] - 213s 17ms/step - loss: 16.3460 - val_loss: 35.0050\n",
      "Epoch 14/200\n",
      "12890/12890 [==============================] - 213s 17ms/step - loss: 16.1715 - val_loss: 35.0093\n",
      "Epoch 15/200\n",
      "12890/12890 [==============================] - 208s 16ms/step - loss: 16.0081 - val_loss: 35.0192\n",
      "Epoch 16/200\n",
      "12890/12890 [==============================] - 211s 16ms/step - loss: 15.8578 - val_loss: 35.0264\n",
      "Epoch 17/200\n",
      "12890/12890 [==============================] - 211s 16ms/step - loss: 15.6893 - val_loss: 35.0376\n",
      "Epoch 18/200\n",
      "12890/12890 [==============================] - 210s 16ms/step - loss: 15.5410 - val_loss: 35.0546\n",
      "Epoch 19/200\n",
      "12890/12890 [==============================] - 203s 16ms/step - loss: 15.3917 - val_loss: 35.0658\n",
      "Epoch 20/200\n",
      "12890/12890 [==============================] - 208s 16ms/step - loss: 15.2514 - val_loss: 35.0811\n",
      "Epoch 21/200\n",
      "12890/12890 [==============================] - 211s 16ms/step - loss: 15.1040 - val_loss: 35.0793\n",
      "Epoch 22/200\n",
      "12890/12890 [==============================] - 189s 15ms/step - loss: 14.9655 - val_loss: 35.0903\n",
      "Epoch 23/200\n",
      "12890/12890 [==============================] - 177s 14ms/step - loss: 14.8211 - val_loss: 35.1010\n",
      "Epoch 24/200\n",
      "12890/12890 [==============================] - 177s 14ms/step - loss: 14.6863 - val_loss: 35.1158\n",
      "Epoch 25/200\n",
      "12890/12890 [==============================] - 178s 14ms/step - loss: 14.5517 - val_loss: 35.1176\n",
      "Epoch 26/200\n",
      "12890/12890 [==============================] - 178s 14ms/step - loss: 14.4230 - val_loss: 35.1263\n",
      "Epoch 27/200\n",
      "12890/12890 [==============================] - 178s 14ms/step - loss: 14.2950 - val_loss: 35.1298\n",
      "Epoch 28/200\n",
      "12890/12890 [==============================] - 178s 14ms/step - loss: 14.1704 - val_loss: 35.1494\n",
      "Epoch 29/200\n",
      "12890/12890 [==============================] - 176s 14ms/step - loss: 14.0436 - val_loss: 35.1520\n",
      "Epoch 30/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 13.9203 - val_loss: 35.1560\n",
      "Epoch 31/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 13.7984 - val_loss: 35.1611\n",
      "Epoch 32/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 13.6788 - val_loss: 35.1688\n",
      "Epoch 33/200\n",
      "12890/12890 [==============================] - 189s 15ms/step - loss: 13.5616 - val_loss: 35.1799\n",
      "Epoch 34/200\n",
      "12890/12890 [==============================] - 220s 17ms/step - loss: 13.4455 - val_loss: 35.1769\n",
      "Epoch 35/200\n",
      "12890/12890 [==============================] - 214s 17ms/step - loss: 13.3270 - val_loss: 35.1744\n",
      "Epoch 36/200\n",
      "12890/12890 [==============================] - 223s 17ms/step - loss: 13.2116 - val_loss: 35.1852\n",
      "Epoch 37/200\n",
      "12890/12890 [==============================] - 219s 17ms/step - loss: 13.1017 - val_loss: 35.1889\n",
      "Epoch 38/200\n",
      "12890/12890 [==============================] - 216s 17ms/step - loss: 12.9906 - val_loss: 35.1871\n",
      "Epoch 39/200\n",
      "12890/12890 [==============================] - 219s 17ms/step - loss: 12.8773 - val_loss: 35.1912\n",
      "Epoch 40/200\n",
      "12890/12890 [==============================] - 217s 17ms/step - loss: 12.7702 - val_loss: 35.1890\n",
      "Epoch 41/200\n",
      "12890/12890 [==============================] - 218s 17ms/step - loss: 12.6611 - val_loss: 35.1871\n",
      "Epoch 42/200\n",
      "12890/12890 [==============================] - 215s 17ms/step - loss: 12.5542 - val_loss: 35.1937\n",
      "Epoch 43/200\n",
      "12890/12890 [==============================] - 218s 17ms/step - loss: 12.4502 - val_loss: 35.1898\n",
      "Epoch 44/200\n",
      "12890/12890 [==============================] - 226s 18ms/step - loss: 12.3503 - val_loss: 35.1940\n",
      "Epoch 45/200\n",
      "12890/12890 [==============================] - 226s 18ms/step - loss: 12.2480 - val_loss: 35.1949\n",
      "Epoch 46/200\n",
      "12890/12890 [==============================] - 228s 18ms/step - loss: 12.1450 - val_loss: 35.1896\n",
      "Epoch 47/200\n",
      "12890/12890 [==============================] - 225s 17ms/step - loss: 12.0502 - val_loss: 35.1898\n",
      "Epoch 48/200\n",
      "12890/12890 [==============================] - 223s 17ms/step - loss: 11.9506 - val_loss: 35.1853\n",
      "Epoch 49/200\n",
      "12890/12890 [==============================] - 223s 17ms/step - loss: 11.8520 - val_loss: 35.1852\n",
      "Epoch 50/200\n",
      "12890/12890 [==============================] - 216s 17ms/step - loss: 11.7572 - val_loss: 35.1821\n",
      "Epoch 51/200\n",
      "12890/12890 [==============================] - 224s 17ms/step - loss: 11.6614 - val_loss: 35.1721\n",
      "Epoch 52/200\n",
      "12890/12890 [==============================] - 217s 17ms/step - loss: 11.5674 - val_loss: 35.1687\n",
      "Epoch 53/200\n",
      "12890/12890 [==============================] - 217s 17ms/step - loss: 11.4747 - val_loss: 35.1548\n",
      "Epoch 54/200\n",
      "12890/12890 [==============================] - 223s 17ms/step - loss: 11.3799 - val_loss: 35.1582\n",
      "Epoch 55/200\n",
      "12890/12890 [==============================] - 226s 18ms/step - loss: 11.2952 - val_loss: 35.1445\n",
      "Epoch 56/200\n",
      "12890/12890 [==============================] - 206s 16ms/step - loss: 11.2054 - val_loss: 35.1424\n",
      "Epoch 57/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 11.1220 - val_loss: 35.1407\n",
      "Epoch 58/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 11.0283 - val_loss: 35.1307\n",
      "Epoch 59/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 10.9425 - val_loss: 35.1222\n",
      "Epoch 60/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 10.8604 - val_loss: 35.1189\n",
      "Epoch 61/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 10.7761 - val_loss: 35.1025\n",
      "Epoch 62/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 10.6930 - val_loss: 35.0995\n",
      "Epoch 63/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 10.6143 - val_loss: 35.0884\n",
      "Epoch 64/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 10.5328 - val_loss: 35.0822\n",
      "Epoch 65/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 10.4548 - val_loss: 35.0770\n",
      "Epoch 66/200\n",
      "12890/12890 [==============================] - 170s 13ms/step - loss: 10.3710 - val_loss: 35.0770\n",
      "Epoch 67/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 10.2964 - val_loss: 35.0719\n",
      "Epoch 68/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 10.2203 - val_loss: 35.0575\n",
      "Epoch 69/200\n",
      "12890/12890 [==============================] - 174s 14ms/step - loss: 10.1417 - val_loss: 35.0530\n",
      "Epoch 70/200\n",
      "12890/12890 [==============================] - 166s 13ms/step - loss: 10.0685 - val_loss: 35.0463\n",
      "Epoch 71/200\n",
      "12890/12890 [==============================] - 174s 13ms/step - loss: 9.9946 - val_loss: 35.0398\n",
      "Epoch 72/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 9.9209 - val_loss: 35.0302\n",
      "Epoch 73/200\n",
      "12890/12890 [==============================] - 161s 12ms/step - loss: 9.8496 - val_loss: 35.0258\n",
      "Epoch 74/200\n",
      "12890/12890 [==============================] - 164s 13ms/step - loss: 9.7750 - val_loss: 35.0205\n",
      "Epoch 75/200\n",
      "12890/12890 [==============================] - 161s 12ms/step - loss: 9.7052 - val_loss: 35.0060\n",
      "Epoch 76/200\n",
      "12890/12890 [==============================] - 158s 12ms/step - loss: 9.6361 - val_loss: 35.0022\n",
      "Epoch 77/200\n",
      "12890/12890 [==============================] - 166s 13ms/step - loss: 9.5662 - val_loss: 34.9972\n",
      "Epoch 78/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 9.4973 - val_loss: 34.9936\n",
      "Epoch 79/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 9.4327 - val_loss: 34.9866\n",
      "Epoch 80/200\n",
      "12890/12890 [==============================] - 166s 13ms/step - loss: 9.3651 - val_loss: 34.9839\n",
      "Epoch 81/200\n",
      "12890/12890 [==============================] - 166s 13ms/step - loss: 9.2988 - val_loss: 34.9783\n",
      "Epoch 82/200\n",
      "12890/12890 [==============================] - 167s 13ms/step - loss: 9.2379 - val_loss: 34.9709\n",
      "Epoch 83/200\n",
      "12890/12890 [==============================] - 157s 12ms/step - loss: 9.1740 - val_loss: 34.9668\n",
      "Epoch 84/200\n",
      "12890/12890 [==============================] - 160s 12ms/step - loss: 9.1101 - val_loss: 34.9656\n",
      "Epoch 85/200\n",
      "12890/12890 [==============================] - 163s 13ms/step - loss: 9.0477 - val_loss: 34.9513\n",
      "Epoch 86/200\n",
      "12890/12890 [==============================] - 163s 13ms/step - loss: 8.9846 - val_loss: 34.9533\n",
      "Epoch 87/200\n",
      "12890/12890 [==============================] - 162s 13ms/step - loss: 8.9282 - val_loss: 34.9439\n",
      "Epoch 88/200\n",
      "12890/12890 [==============================] - 161s 13ms/step - loss: 8.8647 - val_loss: 34.9404\n",
      "Epoch 89/200\n",
      "12890/12890 [==============================] - 160s 12ms/step - loss: 8.8077 - val_loss: 34.9365\n",
      "Epoch 90/200\n",
      "12890/12890 [==============================] - 169s 13ms/step - loss: 8.7492 - val_loss: 34.9315\n",
      "Epoch 91/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 8.6897 - val_loss: 34.9258\n",
      "Epoch 92/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 8.6350 - val_loss: 34.9226\n",
      "Epoch 93/200\n",
      "12890/12890 [==============================] - 164s 13ms/step - loss: 8.5771 - val_loss: 34.9299\n",
      "Epoch 94/200\n",
      "12890/12890 [==============================] - 158s 12ms/step - loss: 8.5242 - val_loss: 34.9281\n",
      "Epoch 95/200\n",
      "12890/12890 [==============================] - 167s 13ms/step - loss: 8.4683 - val_loss: 34.9143\n",
      "Epoch 96/200\n",
      "12890/12890 [==============================] - 176s 14ms/step - loss: 8.4142 - val_loss: 34.9189\n",
      "Epoch 97/200\n",
      "12890/12890 [==============================] - 191s 15ms/step - loss: 8.3605 - val_loss: 34.9030\n",
      "Epoch 98/200\n",
      "12890/12890 [==============================] - 198s 15ms/step - loss: 8.3084 - val_loss: 34.9020\n",
      "Epoch 99/200\n",
      "12890/12890 [==============================] - 190s 15ms/step - loss: 8.2555 - val_loss: 34.8966\n",
      "Epoch 100/200\n",
      "12890/12890 [==============================] - 165s 13ms/step - loss: 8.2034 - val_loss: 34.8932\n",
      "Epoch 101/200\n",
      "12890/12890 [==============================] - 159s 12ms/step - loss: 8.1500 - val_loss: 34.8940\n",
      "Epoch 102/200\n",
      "12890/12890 [==============================] - 164s 13ms/step - loss: 8.0969 - val_loss: 34.8862\n",
      "Epoch 103/200\n",
      "12890/12890 [==============================] - 174s 14ms/step - loss: 8.0480 - val_loss: 34.8865\n",
      "Epoch 104/200\n",
      "12890/12890 [==============================] - 190s 15ms/step - loss: 7.9995 - val_loss: 34.8795\n",
      "Epoch 105/200\n",
      "12890/12890 [==============================] - 177s 14ms/step - loss: 7.9503 - val_loss: 34.8802\n",
      "Epoch 106/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 7.9001 - val_loss: 34.8700\n",
      "Epoch 107/200\n",
      "12890/12890 [==============================] - 169s 13ms/step - loss: 7.8555 - val_loss: 34.8679\n",
      "Epoch 108/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 7.8070 - val_loss: 34.8666\n",
      "Epoch 109/200\n",
      "12890/12890 [==============================] - 188s 15ms/step - loss: 7.7609 - val_loss: 34.8648\n",
      "Epoch 110/200\n",
      "12890/12890 [==============================] - 183s 14ms/step - loss: 7.7143 - val_loss: 34.8604\n",
      "Epoch 111/200\n",
      "12890/12890 [==============================] - 168s 13ms/step - loss: 7.6669 - val_loss: 34.8524\n",
      "Epoch 112/200\n",
      "12890/12890 [==============================] - 159s 12ms/step - loss: 7.6228 - val_loss: 34.8483\n",
      "Epoch 113/200\n",
      "12890/12890 [==============================] - 158s 12ms/step - loss: 7.5778 - val_loss: 34.8493\n",
      "Epoch 114/200\n",
      "12890/12890 [==============================] - 159s 12ms/step - loss: 7.5345 - val_loss: 34.8430\n",
      "Epoch 115/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 7.4903 - val_loss: 34.8452\n",
      "Epoch 116/200\n",
      "12890/12890 [==============================] - 172s 13ms/step - loss: 7.4466 - val_loss: 34.8429\n",
      "Epoch 117/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 7.4064 - val_loss: 34.8413\n",
      "Epoch 118/200\n",
      "12890/12890 [==============================] - 158s 12ms/step - loss: 7.3639 - val_loss: 34.8313\n",
      "Epoch 119/200\n",
      "12890/12890 [==============================] - 157s 12ms/step - loss: 7.3234 - val_loss: 34.8421\n",
      "Epoch 120/200\n",
      "12890/12890 [==============================] - 163s 13ms/step - loss: 7.2812 - val_loss: 34.8338\n",
      "Epoch 121/200\n",
      "12890/12890 [==============================] - 176s 14ms/step - loss: 7.2402 - val_loss: 34.8391\n",
      "Epoch 122/200\n",
      "12890/12890 [==============================] - 173s 13ms/step - loss: 7.2007 - val_loss: 34.8337\n",
      "Epoch 123/200\n",
      "12890/12890 [==============================] - 169s 13ms/step - loss: 7.1614 - val_loss: 34.8321\n",
      "Epoch 124/200\n",
      "12890/12890 [==============================] - 171s 13ms/step - loss: 7.1244 - val_loss: 34.8327\n",
      "Epoch 125/200\n",
      "12890/12890 [==============================] - 168s 13ms/step - loss: 7.0855 - val_loss: 34.8260\n",
      "Epoch 126/200\n",
      "12890/12890 [==============================] - 157s 12ms/step - loss: 7.0474 - val_loss: 34.8225\n",
      "Epoch 127/200\n",
      "12890/12890 [==============================] - 164s 13ms/step - loss: 7.0102 - val_loss: 34.8265\n",
      "Epoch 128/200\n",
      " 2902/12890 [=====>........................] - ETA: 2:24 - loss: 6.9344"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49m[train\u001b[39m.\u001b[39;49muser\u001b[39m.\u001b[39;49mvalues, train\u001b[39m.\u001b[39;49mbook\u001b[39m.\u001b[39;49mvalues], y\u001b[39m=\u001b[39;49mtrain\u001b[39m.\u001b[39;49mbookRating\u001b[39m.\u001b[39;49mvalues, \n\u001b[0;32m      2\u001b[0m                     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[0;32m      3\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m([test\u001b[39m.\u001b[39;49muser\u001b[39m.\u001b[39;49mvalues, test\u001b[39m.\u001b[39;49mbook\u001b[39m.\u001b[39;49mvalues], test\u001b[39m.\u001b[39;49mbookRating\u001b[39m.\u001b[39;49mvalues))\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\engine\\training.py:1691\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1689\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1690\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1691\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1692\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1693\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \n\u001b[0;32m    470\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 475\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    321\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 322\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    325\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    327\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    343\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 345\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    348\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    392\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 393\u001b[0m     hook(batch, logs)\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1093\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1168\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1169\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1170\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\utils\\generic_utils.py:296\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    293\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    295\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[1;32m--> 296\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    297\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\keras\\utils\\io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message)\n\u001b[1;32m---> 80\u001b[0m     sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mflush()\n\u001b[0;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     logging\u001b[39m.\u001b[39minfo(message)\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\ipykernel\\iostream.py:521\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"trigger actual zmq send\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \n\u001b[0;32m    512\u001b[0m \u001b[39msend will happen in the background thread\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    515\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\n\u001b[0;32m    516\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mthread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m ):\n\u001b[0;32m    520\u001b[0m     \u001b[39m# request flush on the background thread\u001b[39;00m\n\u001b[1;32m--> 521\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpub_thread\u001b[39m.\u001b[39;49mschedule(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flush)\n\u001b[0;32m    522\u001b[0m     \u001b[39m# wait for flush to actually get through, if we can.\u001b[39;00m\n\u001b[0;32m    523\u001b[0m     evt \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mEvent()\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\ipykernel\\iostream.py:213\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_events\u001b[39m.\u001b[39mappend(f)\n\u001b[0;32m    212\u001b[0m     \u001b[39m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event_pipe\u001b[39m.\u001b[39;49msend(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     f()\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\zmq\\sugar\\socket.py:620\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    613\u001b[0m         data \u001b[39m=\u001b[39m zmq\u001b[39m.\u001b[39mFrame(\n\u001b[0;32m    614\u001b[0m             data,\n\u001b[0;32m    615\u001b[0m             track\u001b[39m=\u001b[39mtrack,\n\u001b[0;32m    616\u001b[0m             copy\u001b[39m=\u001b[39mcopy \u001b[39mor\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    617\u001b[0m             copy_threshold\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_threshold,\n\u001b[0;32m    618\u001b[0m         )\n\u001b[0;32m    619\u001b[0m     data\u001b[39m.\u001b[39mgroup \u001b[39m=\u001b[39m group\n\u001b[1;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(data, flags\u001b[39m=\u001b[39;49mflags, copy\u001b[39m=\u001b[39;49mcopy, track\u001b[39m=\u001b[39;49mtrack)\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:746\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:793\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\backend\\cython\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\dimgr\\finalfinal\\rec_env\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x=[train.user.values, train.book.values], y=train.bookRating.values, \n",
    "                    batch_size=64, epochs=200, verbose=1, \n",
    "                    validation_data=([test.user.values, test.book.values], test.bookRating.values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6445/6445 [==============================] - 3s 527us/step - loss: 20.1628\n",
      "Test MSE: 20.16280746459961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model\n",
    "mse = model.evaluate([test.user.values, test.book.values], test.bookRating.values)\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Data/BX_book/TF_boork-rec.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
