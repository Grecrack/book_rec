{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet imports necessary libraries and modules for a machine learning task. Here's a description of each import:\n",
    "\n",
    "- `numpy` (imported as `np`): A library for numerical operations and array manipulation in Python.\n",
    "- `pandas` (imported as `pd`): A library for data manipulation and analysis, providing data structures and functions to work with structured data.\n",
    "- `scipy.stats`: A module from SciPy, a scientific computing library in Python, providing statistical functions and distributions.\n",
    "- `sklearn.model_selection`: A module from scikit-learn, a popular machine learning library in Python, used for splitting data into training and testing sets.\n",
    "- `keras.models`: A module from Keras, a deep learning library, used for defining and training models.\n",
    "- `keras.layers`: A module from Keras, used for constructing the layers of a neural network model.\n",
    "- `keras.optimizers`: A module from Keras, providing various optimization algorithms for training neural networks.\n",
    "- `keras.callbacks`: A module from Keras, containing callbacks that can be used during model training.\n",
    "- `keras.losses`: A module from Keras, providing different loss functions for regression and classification tasks.\n",
    "\n",
    "This code snippet sets up the necessary imports for working with Keras and other required libraries in a machine learning project. These libraries provide essential functionality for data manipulation, model construction, optimization, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import MeanAbsoluteError, MeanSquaredError, Huber, LogCosh"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Here we are loading the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=pd.read_csv('../Data/users.csv')\n",
    "ratings=pd.read_csv('../Data/ratings.csv')\n",
    "books=pd.read_csv('../Data/books.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user-id and book-id mapping\n",
    "\n",
    "The following code snippet explains the purpose of each line:\n",
    "\n",
    "1. **Get Unique User IDs**: Retrieves the unique user IDs from the 'user_id' column of the ratings DataFrame and converts them to a list using the `tolist()` function. The user IDs are stored in the `user_ids` variable.\n",
    "\n",
    "2. **Create User ID to Index Mapping**: Creates a dictionary, `user2user_encoded`, to map each user ID to its corresponding index. The dictionary comprehension iterates over the `user_ids` list, assigning an index (starting from 0) to each user ID using the `enumerate()` function. The resulting dictionary maps each user ID to its index.\n",
    "\n",
    "3. **Create Index to User ID Mapping**: Creates a dictionary, `userencoded2user`, to map each index to its corresponding user ID. The dictionary comprehension performs the reverse mapping, iterating over the enumerated indices and assigning each index to its corresponding user ID.\n",
    "\n",
    "4. **Get Unique Book IDs**: Retrieves the unique book IDs from the 'book_id' column of the ratings DataFrame and converts them to a list using the `tolist()` function. The book IDs are stored in the `book_ids` variable.\n",
    "\n",
    "5. **Create Book ID to Index Mapping**: Creates a dictionary, `book2book_encoded`, to map each book ID to its corresponding index. Similar to the user ID mapping, the dictionary comprehension assigns an index (starting from 0) to each book ID using the `enumerate()` function.\n",
    "\n",
    "6. **Create Index to Book ID Mapping**: Creates a dictionary, `book_encoded2book`, to map each index to its corresponding book ID. The dictionary comprehension performs the reverse mapping, assigning each index to its corresponding book ID.\n",
    "\n",
    "These mappings between user and book IDs and their corresponding indices are essential when working with embedding layers in recommendation systems. They provide a convenient way to convert between raw IDs and indices during model training and prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = ratings['user_id'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "book_ids = ratings['book_id'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map user-id and book-ids to user and book indices\n",
    "\n",
    "1. **Map User IDs to Indices**: Adds a new column called 'user' to the ratings DataFrame by mapping the 'user_id' column to the corresponding user indices using the `map()` function with the `user2user_encoded` dictionary. Each user ID in the 'user_id' column is replaced with its corresponding index value.\n",
    "\n",
    "2. **Map Book IDs to Indices**: Adds a new column called 'book' to the ratings DataFrame by mapping the 'book_id' column to the corresponding book indices using the `map()` function with the `book2book_encoded` dictionary. Each book ID in the 'book_id' column is replaced with its corresponding index value.\n",
    "\n",
    "By performing these mapping operations, the user and book IDs are transformed into their respective indices, which are necessary for feeding the data into the embedding layers of the recommendation model. This allows the model to work with indices instead of raw IDs, enabling efficient computations and improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['user'] = ratings['user_id'].map(user2user_encoded)\n",
    "ratings['book'] = ratings['book_id'].map(book2book_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and testing set\n",
    "\n",
    "1. **Split Data into Train and Test Sets**: Splits the ratings DataFrame into train and test sets using the `train_test_split()` function from scikit-learn. The `train_test_split()` function takes the input data (ratings) and splits it into two subsets based on the specified `test_size` parameter. In this case, the test set will have a size of 20% of the entire dataset.\n",
    "\n",
    "2. **Assign Train and Test Sets**: Assigns the train and test sets to the variables `train` and `test`, respectively. The train set will contain 80% of the data, while the test set will contain 20% of the data.\n",
    "\n",
    "By splitting the data into train and test sets, we create separate subsets that can be used for training and evaluating the recommendation model. The train set is used to train the model, while the test set is used to evaluate its performance and generalization on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of users and books\n",
    "\n",
    "\n",
    "1. **Get the Number of Users**: Calculates the number of unique users in the dataset by taking the length of the `user2user_encoded` dictionary. The `len()` function returns the number of elements in the dictionary, which represents the total number of unique users.\n",
    "\n",
    "2. **Get the Number of Books**: Calculates the number of unique books in the dataset by taking the length of the `book_encoded2book` dictionary. Similarly, the `len()` function returns the number of elements in the dictionary, which represents the total number of unique books.\n",
    "\n",
    "By obtaining the number of users and books, we can determine the dimensions of the embedding layers in the recommendation model. The number of users corresponds to the number of unique user indices, and the number of books corresponds to the number of unique book indices. These values are essential for setting the correct dimensions of the embedding layers to capture the user and book representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set embedding dimension\n",
    "\n",
    "1. **Set the Embedding Dimension**: Specifies the dimensionality of the embedding vectors in the recommendation model. The `embedding_dim` variable is set to 10, which means each user and book will be represented by a vector of length 10 in the embedding space.\n",
    "\n",
    "By setting the embedding dimension, we determine the size of the vector representations for users and books. A higher embedding dimension may allow for more expressive representations but can also increase the model's complexity and resource requirements. On the other hand, a lower embedding dimension may result in more compact representations but may also limit the model's ability to capture intricate user-item interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "We're using Keras Functional API to build a model with Embedding layers for users and books.  \n",
    "These embeddings will learn to represent user preferences and book properties during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Define Input Layers**: Creates two input layers, `user_input` and `book_input`, which correspond to the user and book inputs, respectively. These input layers define the shape of the input data.\n",
    "\n",
    "2. **Embedding Layers**: Creates embedding layers for users and books using the `Embedding` class. The embedding layers map the user and book indices to their corresponding embedding vectors in the latent space. The dimensions of the embedding layers are determined by the number of users and books (`num_users` and `num_books`) and the specified `embedding_dim`.\n",
    "\n",
    "3. **Flatten Layers**: Flattens the user and book embedding layers using the `Flatten` class. This step converts the 2D tensor outputs from the embedding layers into 1D vectors.\n",
    "\n",
    "4. **Concatenate Layers**: Concatenates the flattened user and book embeddings using the `Concatenate` class. This step combines the user and book representations into a single vector that captures the interactions between users and books.\n",
    "\n",
    "5. **Dense Layers**: Adds a dense layer with 16 units and ReLU activation function on top of the concatenated layer. This layer learns higher-level representations based on the combined user and book information.\n",
    "\n",
    "6. **Output Layer**: Adds a dense layer with 1 unit and linear activation function as the output layer. This layer predicts the rating for a given user-book pair.\n",
    "\n",
    "7. **Create the Model**: Creates an instance of the `Model` class, specifying the input and output layers. This model defines the architecture for the recommendation system.\n",
    "\n",
    "8. **Return the Model**: Returns the created model.\n",
    "\n",
    "This function encapsulates the creation of the recommendation model with its layers and architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    user_input = Input(shape=(1,))\n",
    "    book_input = Input(shape=(1,))\n",
    "\n",
    "    user_embedding = Embedding(num_users, embedding_dim)(user_input)\n",
    "    book_embedding = Embedding(num_books, embedding_dim)(book_input)\n",
    "\n",
    "    user_flatten = Flatten()(user_embedding)\n",
    "    book_flatten = Flatten()(book_embedding)\n",
    "\n",
    "    concatenated = Concatenate()([user_flatten, book_flatten])\n",
    "\n",
    "    dense_1 = Dense(16, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='linear')(dense_1)\n",
    "\n",
    "    model = Model(inputs=[user_input, book_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model` function performs the following steps to train the recommendation model:\n",
    "\n",
    "1. **Compile the Model**: Compiles the recommendation model with the specified `loss_function` and `optimizer`. This step configures the model for training by defining the loss function to optimize and the optimizer algorithm to use.\n",
    "\n",
    "2. **Fit the Model**: Fits the recommendation model to the training data using the `fit` method. It specifies the training inputs (`[train.user.values, train.book.values]`), the training targets (`train.rating.values`), and other parameters such as `batch_size`, `epochs`, and `verbose`. This step trains the model on the training data for the specified number of epochs.\n",
    "\n",
    "3. **Validate the Model**: Evaluates the trained model on the validation data (`[test.user.values, test.book.values]` and `test.rating.values`) during the training process. This provides insights into the model's performance on unseen data and helps in monitoring its progress.\n",
    "\n",
    "4. **Save the Model**: Saves the trained model to the specified `model_save_path` using the `ModelCheckpoint` callback. This ensures that only the best model based on the validation loss is saved.\n",
    "\n",
    "5. **Return the Validation Loss**: Returns the validation loss (`val_loss`) from the history of the model. The validation loss provides an indication of how well the model is generalizing to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(batch_size, optimizer, loss_function, model_save_path, num_epochs):\n",
    "    checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                        batch_size=batch_size, epochs=num_epochs, verbose=1,\n",
    "                        validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                        callbacks=[checkpoint])\n",
    "    model.save(model_save_path)\n",
    "    checkpoint = ModelCheckpoint(model_save_path, monitor='val_loss', save_best_only=True, mode='min')\n",
    "    return history.history['val_loss'][-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tuning the Recommendation Model</b>\n",
    "\n",
    "- **Optimizers**: A list that contains instances of different optimizers, including Adam and RMSprop. Optimizers are responsible for updating the model's weights during training to minimize the loss function.\n",
    "\n",
    "- **Loss Functions**: A list that contains instances of different loss functions, such as MeanAbsoluteError, MeanSquaredError, Huber, and LogCosh. Loss functions quantify the difference between predicted ratings and actual ratings.\n",
    "\n",
    "- **Batch Sizes**: A list that contains different batch sizes, such as 32 and 64, which determine the number of samples processed before updating the model's weights.\n",
    "\n",
    "- **Model Paths**: An empty list that will store the paths of the best models found during the tuning process.\n",
    "\n",
    "- **Best Validation Loss**: A variable initialized with a large value (`float('inf')`) to track the best validation loss achieved during training.\n",
    "\n",
    "- **Best Optimizer**: A variable that will store the optimizer yielding the best validation loss.\n",
    "\n",
    "- **Best Loss Function**: A variable that will store the loss function yielding the best validation loss.\n",
    "\n",
    "- **Best Batch Size**: A variable that will store the batch size yielding the best validation loss.\n",
    "\n",
    "- **Best Number of Epochs**: A variable that will store the number of epochs corresponding to the best validation loss.\n",
    "\n",
    "- **Best Model Path**: A variable that will store the path of the model with the best validation loss.\n",
    "\n",
    "These variables and lists will be used to track and update the best hyperparameters and model performance during the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [Adam(), RMSprop()]\n",
    "loss_functions = [MeanAbsoluteError(), MeanSquaredError(), Huber(), LogCosh()]\n",
    "batch_sizes = [32, 64]\n",
    "model_paths = []\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_optimizer = None\n",
    "best_loss_function = None\n",
    "best_batch_size = None\n",
    "best_num_epochs = None\n",
    "best_model_path = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tuning Hyperparameters and Training the Recommendation Model</b>\n",
    "\n",
    "1. **Optimizer Loop**: The outer loop iterates over the optimizers in the `optimizers` list.\n",
    "\n",
    "2. **Loss Function Loop**: The second loop iterates over the loss functions in the `loss_functions` list.\n",
    "\n",
    "3. **Batch Size Loop**: The innermost loop iterates over the batch sizes in the `batch_sizes` list.\n",
    "\n",
    "4. **Number of Epochs**: The `num_epochs` variable is set to 20, indicating the number of training epochs for each combination of hyperparameters.\n",
    "\n",
    "5. **Create Model**: A new instance of the recommendation model is created using the `create_model()` function.\n",
    "\n",
    "6. **Model Save Path**: The `model_save_path` variable is generated to specify the path for saving the trained model based on the current combination of optimizer, loss function, and batch size.\n",
    "\n",
    "7. **Print Hyperparameters**: The optimizer, loss function, and batch size are printed for tracking the progress of the tuning process.\n",
    "\n",
    "8. **Train the Model**: The `train_model()` function is called to train the model with the current hyperparameters. The function takes the batch size, optimizer, loss function, model save path, and number of epochs as input arguments. It returns the validation loss of the trained model.\n",
    "\n",
    "9. **Update Best Hyperparameters**: If the validation loss obtained with the current hyperparameters is lower than the previous best validation loss (`best_val_loss`), the best hyperparameters and model information are updated.\n",
    "\n",
    "10. **Best Model Path**: The `best_model_path` variable stores the path of the model with the lowest validation loss among all the combinations.\n",
    "\n",
    "By iterating over different combinations of optimizers, loss functions, and batch sizes, the code finds the best hyperparameters for training the recommendation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.MeanAbsoluteError object at 0x0000022082DC6CE0> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 48s 2ms/step - loss: 0.7078 - val_loss: 0.6346\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 48s 2ms/step - loss: 0.6066 - val_loss: 0.6110\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 53s 3ms/step - loss: 0.5731 - val_loss: 0.6021\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 49s 3ms/step - loss: 0.5560 - val_loss: 0.6044\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 53s 3ms/step - loss: 0.5452 - val_loss: 0.6009\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 36s 2ms/step - loss: 0.5378 - val_loss: 0.6024\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5322 - val_loss: 0.6038\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5274 - val_loss: 0.6045\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5238 - val_loss: 0.6040\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5197 - val_loss: 0.6036\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5166 - val_loss: 0.6062\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5133 - val_loss: 0.6080\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5107 - val_loss: 0.6119\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5082 - val_loss: 0.6099\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5059 - val_loss: 0.6174\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.5038 - val_loss: 0.6113\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 48s 2ms/step - loss: 0.5025 - val_loss: 0.6146\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 47s 2ms/step - loss: 0.5004 - val_loss: 0.6121\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 48s 2ms/step - loss: 0.4987 - val_loss: 0.6115\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 48s 2ms/step - loss: 0.4970 - val_loss: 0.6168\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.MeanAbsoluteError object at 0x0000022082DC6CE0> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.6799 - val_loss: 0.6349\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.6139 - val_loss: 0.6222\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.5937 - val_loss: 0.6156\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.5795 - val_loss: 0.6150\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 21s 2ms/step - loss: 0.5694 - val_loss: 0.6128\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5611 - val_loss: 0.6110\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5543 - val_loss: 0.6109\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5486 - val_loss: 0.6122\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5436 - val_loss: 0.6120\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5390 - val_loss: 0.6159\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5350 - val_loss: 0.6166\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5311 - val_loss: 0.6139\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5274 - val_loss: 0.6152\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5240 - val_loss: 0.6154\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5206 - val_loss: 0.6164\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5174 - val_loss: 0.6180\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5146 - val_loss: 0.6185\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5117 - val_loss: 0.6195\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5088 - val_loss: 0.6208\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.5062 - val_loss: 0.6228\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.MeanSquaredError object at 0x0000022082DC6F50> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.7689 - val_loss: 0.7013\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.6756 - val_loss: 0.6903\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.6515 - val_loss: 0.6905\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 43s 2ms/step - loss: 0.6349 - val_loss: 0.6865\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 52s 3ms/step - loss: 0.6213 - val_loss: 0.6893\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 51s 3ms/step - loss: 0.6084 - val_loss: 0.6910\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.5955 - val_loss: 0.6913\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 50s 3ms/step - loss: 0.5832 - val_loss: 0.6949\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.5715 - val_loss: 0.7002\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 36s 2ms/step - loss: 0.5604 - val_loss: 0.7030\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5497 - val_loss: 0.7121\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5391 - val_loss: 0.7120\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5289 - val_loss: 0.7254\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5187 - val_loss: 0.7329\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.5087 - val_loss: 0.7298\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.4993 - val_loss: 0.7374\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.4901 - val_loss: 0.7500\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.4814 - val_loss: 0.7517\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.4730 - val_loss: 0.7609\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.4651 - val_loss: 0.7735\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.MeanSquaredError object at 0x0000022082DC6F50> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 16s 2ms/step - loss: 0.8286 - val_loss: 0.7010\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.6715 - val_loss: 0.6854\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.6435 - val_loss: 0.6844\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.6215 - val_loss: 0.6870\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.6023 - val_loss: 0.6905\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.5847 - val_loss: 0.6952\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.5679 - val_loss: 0.7027\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.5522 - val_loss: 0.7094\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.5373 - val_loss: 0.7198\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.5231 - val_loss: 0.7310\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.5108 - val_loss: 0.7375\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4990 - val_loss: 0.7487\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.4884 - val_loss: 0.7525\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.4789 - val_loss: 0.7575\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4703 - val_loss: 0.7667\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4624 - val_loss: 0.7734\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4548 - val_loss: 0.7829\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4481 - val_loss: 0.7885\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4414 - val_loss: 0.7994\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.4356 - val_loss: 0.8019\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.Huber object at 0x0000022082DC7F70> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.3313 - val_loss: 0.3072\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.2951 - val_loss: 0.3014\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.2854 - val_loss: 0.3014\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.2770 - val_loss: 0.2996\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 51s 3ms/step - loss: 0.2696 - val_loss: 0.3010\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 46s 2ms/step - loss: 0.2624 - val_loss: 0.3024\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 51s 3ms/step - loss: 0.2553 - val_loss: 0.3037\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 51s 3ms/step - loss: 0.2486 - val_loss: 0.3074\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 39s 2ms/step - loss: 0.2424 - val_loss: 0.3108\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2366 - val_loss: 0.3130\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2313 - val_loss: 0.3161\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2264 - val_loss: 0.3167\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2220 - val_loss: 0.3207\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2178 - val_loss: 0.3214\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2139 - val_loss: 0.3242\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2104 - val_loss: 0.3273\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2070 - val_loss: 0.3315\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2039 - val_loss: 0.3299\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2008 - val_loss: 0.3340\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 37s 2ms/step - loss: 0.1982 - val_loss: 0.3357\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.Huber object at 0x0000022082DC7F70> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.3337 - val_loss: 0.3062\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2949 - val_loss: 0.3016\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 24s 2ms/step - loss: 0.2874 - val_loss: 0.2993\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2801 - val_loss: 0.2990\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.2725 - val_loss: 0.3011\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2651 - val_loss: 0.3013\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.2581 - val_loss: 0.3036\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2515 - val_loss: 0.3068\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 26s 3ms/step - loss: 0.2452 - val_loss: 0.3092\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2391 - val_loss: 0.3109\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 23s 2ms/step - loss: 0.2333 - val_loss: 0.3143\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 19s 2ms/step - loss: 0.2280 - val_loss: 0.3160\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2229 - val_loss: 0.3192\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2181 - val_loss: 0.3212\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2135 - val_loss: 0.3234\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2094 - val_loss: 0.3264\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2055 - val_loss: 0.3315\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2017 - val_loss: 0.3333\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1983 - val_loss: 0.3346\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1952 - val_loss: 0.3370\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.LogCosh object at 0x0000022082DC6FB0> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 28s 1ms/step - loss: 0.2951 - val_loss: 0.2721\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2626 - val_loss: 0.2683\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2545 - val_loss: 0.2662\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2468 - val_loss: 0.2664\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2398 - val_loss: 0.2679\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.2336 - val_loss: 0.2701\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 32s 2ms/step - loss: 0.2279 - val_loss: 0.2731\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.2228 - val_loss: 0.2741\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 50s 3ms/step - loss: 0.2180 - val_loss: 0.2769\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.2133 - val_loss: 0.2779\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.2090 - val_loss: 0.2792\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 50s 3ms/step - loss: 0.2048 - val_loss: 0.2823\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.2011 - val_loss: 0.2863\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1976 - val_loss: 0.2866\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1943 - val_loss: 0.2876\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1915 - val_loss: 0.2907\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1887 - val_loss: 0.2937\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1861 - val_loss: 0.2956\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 45s 2ms/step - loss: 0.1838 - val_loss: 0.2953\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 40s 2ms/step - loss: 0.1815 - val_loss: 0.2978\n",
      "<keras.optimizers.optimizer_v2.adam.Adam object at 0x0000022082DC6EC0> <keras.losses.LogCosh object at 0x0000022082DC6FB0> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.3107 - val_loss: 0.2721\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2622 - val_loss: 0.2676\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2548 - val_loss: 0.2665\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2485 - val_loss: 0.2672\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2428 - val_loss: 0.2677\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2375 - val_loss: 0.2696\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2322 - val_loss: 0.2710\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2272 - val_loss: 0.2724\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2224 - val_loss: 0.2750\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2176 - val_loss: 0.2771\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2132 - val_loss: 0.2792\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2090 - val_loss: 0.2820\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2049 - val_loss: 0.2846\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.2010 - val_loss: 0.2862\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1974 - val_loss: 0.2876\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1939 - val_loss: 0.2909\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1905 - val_loss: 0.2919\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1874 - val_loss: 0.2943\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 15s 2ms/step - loss: 0.1844 - val_loss: 0.2971\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 14s 1ms/step - loss: 0.1816 - val_loss: 0.2993\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.MeanAbsoluteError object at 0x0000022082DC6CE0> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 31s 2ms/step - loss: 0.7200 - val_loss: 0.6450\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.6234 - val_loss: 0.6318\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.6128 - val_loss: 0.6309\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.6081 - val_loss: 0.6267\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 39s 2ms/step - loss: 0.6048 - val_loss: 0.6280\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.6007 - val_loss: 0.6249\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 39s 2ms/step - loss: 0.5965 - val_loss: 0.6241\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 39s 2ms/step - loss: 0.5927 - val_loss: 0.6190\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 32s 2ms/step - loss: 0.5892 - val_loss: 0.6200\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5866 - val_loss: 0.6207\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5842 - val_loss: 0.6180\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5817 - val_loss: 0.6204\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5792 - val_loss: 0.6236\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5774 - val_loss: 0.6183\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5709 - val_loss: 0.6048\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5564 - val_loss: 0.6021\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5504 - val_loss: 0.6020\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5466 - val_loss: 0.6039\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.5440 - val_loss: 0.6060\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.5420 - val_loss: 0.6073\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.MeanAbsoluteError object at 0x0000022082DC6CE0> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.7669 - val_loss: 0.6460\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6273 - val_loss: 0.6349\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 15s 2ms/step - loss: 0.6173 - val_loss: 0.6319\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6121 - val_loss: 0.6301\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6082 - val_loss: 0.6274\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6050 - val_loss: 0.6252\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6024 - val_loss: 0.6257\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 16s 2ms/step - loss: 0.6001 - val_loss: 0.6228\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5977 - val_loss: 0.6219\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5942 - val_loss: 0.6208\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5887 - val_loss: 0.6170\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 19s 2ms/step - loss: 0.5825 - val_loss: 0.6170\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5783 - val_loss: 0.6152\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5745 - val_loss: 0.6130\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5704 - val_loss: 0.6136\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5660 - val_loss: 0.6116\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5620 - val_loss: 0.6123\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5587 - val_loss: 0.6110\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 16s 2ms/step - loss: 0.5561 - val_loss: 0.6102\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.5537 - val_loss: 0.6118\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.MeanSquaredError object at 0x0000022082DC6F50> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 26s 1ms/step - loss: 0.9310 - val_loss: 0.7243\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6959 - val_loss: 0.7160\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6821 - val_loss: 0.7061\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6749 - val_loss: 0.7063\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6697 - val_loss: 0.7007\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6647 - val_loss: 0.7021\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6601 - val_loss: 0.7057\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6562 - val_loss: 0.6994\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6520 - val_loss: 0.7024\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6478 - val_loss: 0.6984\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6437 - val_loss: 0.7007\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.6396 - val_loss: 0.7030\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 27s 1ms/step - loss: 0.6352 - val_loss: 0.6996\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6305 - val_loss: 0.7129\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6259 - val_loss: 0.7029\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6215 - val_loss: 0.7007\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6175 - val_loss: 0.7100\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 32s 2ms/step - loss: 0.6130 - val_loss: 0.7109\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6085 - val_loss: 0.7064\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 33s 2ms/step - loss: 0.6043 - val_loss: 0.7074\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.MeanSquaredError object at 0x0000022082DC6F50> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 18s 2ms/step - loss: 1.1028 - val_loss: 0.7228\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6947 - val_loss: 0.7094\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.6801 - val_loss: 0.7060\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 15s 2ms/step - loss: 0.6732 - val_loss: 0.7029\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6684 - val_loss: 0.7015\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6642 - val_loss: 0.7010\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6588 - val_loss: 0.6987\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6533 - val_loss: 0.6996\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6486 - val_loss: 0.6988\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6443 - val_loss: 0.6990\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6404 - val_loss: 0.6978\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6366 - val_loss: 0.7003\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6331 - val_loss: 0.7100\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6294 - val_loss: 0.6990\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6257 - val_loss: 0.7027\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6222 - val_loss: 0.7028\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6191 - val_loss: 0.7037\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6160 - val_loss: 0.7072\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6127 - val_loss: 0.7080\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.6099 - val_loss: 0.7093\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.Huber object at 0x0000022082DC7F70> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.3814 - val_loss: 0.3168\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.3043 - val_loss: 0.3111\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.2989 - val_loss: 0.3097\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 26s 1ms/step - loss: 0.2965 - val_loss: 0.3082\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2953 - val_loss: 0.3089\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2942 - val_loss: 0.3080\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2933 - val_loss: 0.3085\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2925 - val_loss: 0.3108\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2915 - val_loss: 0.3075\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 40s 2ms/step - loss: 0.2882 - val_loss: 0.3060\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2861 - val_loss: 0.3058\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2842 - val_loss: 0.3045\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 29s 1ms/step - loss: 0.2825 - val_loss: 0.3068\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.2812 - val_loss: 0.3080\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.2802 - val_loss: 0.3053\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.2791 - val_loss: 0.3078\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.2781 - val_loss: 0.3061\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.2768 - val_loss: 0.3075\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 25s 1ms/step - loss: 0.2754 - val_loss: 0.3082\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 24s 1ms/step - loss: 0.2741 - val_loss: 0.3059\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.Huber object at 0x0000022082DC7F70> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.4236 - val_loss: 0.3145\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.3023 - val_loss: 0.3083\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2963 - val_loss: 0.3078\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2934 - val_loss: 0.3059\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2915 - val_loss: 0.3050\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2899 - val_loss: 0.3051\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2883 - val_loss: 0.3041\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2867 - val_loss: 0.3048\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2851 - val_loss: 0.3042\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 20s 2ms/step - loss: 0.2836 - val_loss: 0.3041\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2822 - val_loss: 0.3037\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 18s 2ms/step - loss: 0.2808 - val_loss: 0.3045\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2795 - val_loss: 0.3050\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2783 - val_loss: 0.3036\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 18s 2ms/step - loss: 0.2771 - val_loss: 0.3040\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2762 - val_loss: 0.3044\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 20s 2ms/step - loss: 0.2753 - val_loss: 0.3046\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2746 - val_loss: 0.3057\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 17s 2ms/step - loss: 0.2739 - val_loss: 0.3065\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 18s 2ms/step - loss: 0.2732 - val_loss: 0.3047\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.LogCosh object at 0x0000022082DC6FB0> 32\n",
      "Epoch 1/20\n",
      "19706/19706 [==============================] - 36s 2ms/step - loss: 0.3358 - val_loss: 0.2796\n",
      "Epoch 2/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2694 - val_loss: 0.2745\n",
      "Epoch 3/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2636 - val_loss: 0.2735\n",
      "Epoch 4/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2609 - val_loss: 0.2726\n",
      "Epoch 5/20\n",
      "19706/19706 [==============================] - 40s 2ms/step - loss: 0.2592 - val_loss: 0.2712\n",
      "Epoch 6/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2575 - val_loss: 0.2712\n",
      "Epoch 7/20\n",
      "19706/19706 [==============================] - 40s 2ms/step - loss: 0.2563 - val_loss: 0.2711\n",
      "Epoch 8/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2551 - val_loss: 0.2702\n",
      "Epoch 9/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2538 - val_loss: 0.2705\n",
      "Epoch 10/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2526 - val_loss: 0.2703\n",
      "Epoch 11/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2515 - val_loss: 0.2707\n",
      "Epoch 12/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2505 - val_loss: 0.2696\n",
      "Epoch 13/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2492 - val_loss: 0.2724\n",
      "Epoch 14/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2478 - val_loss: 0.2698\n",
      "Epoch 15/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2456 - val_loss: 0.2698\n",
      "Epoch 16/20\n",
      "19706/19706 [==============================] - 35s 2ms/step - loss: 0.2433 - val_loss: 0.2709\n",
      "Epoch 17/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2412 - val_loss: 0.2701\n",
      "Epoch 18/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2394 - val_loss: 0.2706\n",
      "Epoch 19/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2376 - val_loss: 0.2729\n",
      "Epoch 20/20\n",
      "19706/19706 [==============================] - 34s 2ms/step - loss: 0.2361 - val_loss: 0.2719\n",
      "<keras.optimizers.optimizer_v2.rmsprop.RMSprop object at 0x0000022082DC79D0> <keras.losses.LogCosh object at 0x0000022082DC6FB0> 64\n",
      "Epoch 1/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.3675 - val_loss: 0.2783\n",
      "Epoch 2/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2682 - val_loss: 0.2738\n",
      "Epoch 3/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2630 - val_loss: 0.2720\n",
      "Epoch 4/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2604 - val_loss: 0.2712\n",
      "Epoch 5/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2584 - val_loss: 0.2706\n",
      "Epoch 6/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2567 - val_loss: 0.2709\n",
      "Epoch 7/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2556 - val_loss: 0.2716\n",
      "Epoch 8/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2548 - val_loss: 0.2707\n",
      "Epoch 9/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2539 - val_loss: 0.2722\n",
      "Epoch 10/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2527 - val_loss: 0.2697\n",
      "Epoch 11/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2511 - val_loss: 0.2707\n",
      "Epoch 12/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2494 - val_loss: 0.2692\n",
      "Epoch 13/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2476 - val_loss: 0.2698\n",
      "Epoch 14/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2460 - val_loss: 0.2697\n",
      "Epoch 15/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2443 - val_loss: 0.2695\n",
      "Epoch 16/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2428 - val_loss: 0.2699\n",
      "Epoch 17/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2414 - val_loss: 0.2702\n",
      "Epoch 18/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2400 - val_loss: 0.2712\n",
      "Epoch 19/20\n",
      "9853/9853 [==============================] - 13s 1ms/step - loss: 0.2387 - val_loss: 0.2721\n",
      "Epoch 20/20\n",
      "9853/9853 [==============================] - 12s 1ms/step - loss: 0.2374 - val_loss: 0.2724\n"
     ]
    }
   ],
   "source": [
    "for optimizer in optimizers:\n",
    "    for loss_function in loss_functions:\n",
    "        for batch_size in batch_sizes:\n",
    "            num_epochs = 20\n",
    "            model = create_model()\n",
    "            model_save_path = f\"model_{optimizer.__class__.__name__}_{loss_function.__class__.__name__}_batch{batch_size}.h5\"\n",
    "            model_paths.append(model_save_path)\n",
    "            print(optimizer, loss_function, batch_size)\n",
    "            val_loss = train_model(batch_size, optimizer, loss_function, model_save_path, num_epochs)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_optimizer = optimizer\n",
    "                best_loss_function = loss_function\n",
    "                best_batch_size = batch_size\n",
    "                best_num_epochs = num_epochs\n",
    "                best_model_path = model_save_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Best Optimizer**: The `best_optimizer` variable holds the optimizer object with the lowest validation loss. This line prints the class name of the best optimizer using the `__class__.__name__` attribute.\n",
    "\n",
    "2. **Best Loss Function**: The `best_loss_function` variable holds the loss function object with the lowest validation loss. This line prints the class name of the best loss function using the `__class__.__name__` attribute.\n",
    "\n",
    "3. **Best Batch Size**: The `best_batch_size` variable stores the batch size value that yielded the lowest validation loss.\n",
    "\n",
    "4. **Best Number of Epochs**: The `best_num_epochs` variable stores the number of epochs used for training the model with the best hyperparameters.\n",
    "\n",
    "5. **Best Model Path**: The `best_model_path` variable contains the file path where the best-trained model is saved.\n",
    "\n",
    "By printing this information, you can easily track the best hyperparameters and model details for further analysis and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best optimizer: RMSprop\n",
      "Best loss function: LogCosh\n",
      "Best batch size: 32\n",
      "Best number of epochs: 20\n",
      "Best model path: model_RMSprop_LogCosh_batch32.h5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best optimizer: {best_optimizer.__class__.__name__}\")\n",
    "print(f\"Best loss function: {best_loss_function.__class__.__name__}\")\n",
    "print(f\"Best batch size: {best_batch_size}\")\n",
    "print(f\"Best number of epochs: {best_num_epochs}\")\n",
    "print(f\"Best model path: {best_model_path}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models\n",
    "We evaluate our trained models on the test data to see which one perfomrs better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.6168\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.6228\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.7735\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.8019\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.3357\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.3370\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.2978\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 7s 1ms/step - loss: 0.2993\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.6073\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.6118\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.7074\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.7093\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 7s 1ms/step - loss: 0.3059\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 7s 1ms/step - loss: 0.3047\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.2719\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "4927/4927 [==============================] - 6s 1ms/step - loss: 0.2724\n",
      "Rank 1: Test MSE for model_RMSprop_LogCosh_batch32.h5: 0.271904319524765\n",
      "Rank 2: Test MSE for model_RMSprop_LogCosh_batch64.h5: 0.27237263321876526\n",
      "Rank 3: Test MSE for model_Adam_LogCosh_batch32.h5: 0.2978402078151703\n",
      "Rank 4: Test MSE for model_Adam_LogCosh_batch64.h5: 0.2992510497570038\n",
      "Rank 5: Test MSE for model_RMSprop_Huber_batch64.h5: 0.3046792447566986\n",
      "Rank 6: Test MSE for model_RMSprop_Huber_batch32.h5: 0.30590736865997314\n",
      "Rank 7: Test MSE for model_Adam_Huber_batch32.h5: 0.3356505036354065\n",
      "Rank 8: Test MSE for model_Adam_Huber_batch64.h5: 0.3369615375995636\n",
      "Rank 9: Test MSE for model_RMSprop_MeanAbsoluteError_batch32.h5: 0.6072995066642761\n",
      "Rank 10: Test MSE for model_RMSprop_MeanAbsoluteError_batch64.h5: 0.6118293404579163\n",
      "Rank 11: Test MSE for model_Adam_MeanAbsoluteError_batch32.h5: 0.616817057132721\n",
      "Rank 12: Test MSE for model_Adam_MeanAbsoluteError_batch64.h5: 0.6228122115135193\n",
      "Rank 13: Test MSE for model_RMSprop_MeanSquaredError_batch32.h5: 0.7074137330055237\n",
      "Rank 14: Test MSE for model_RMSprop_MeanSquaredError_batch64.h5: 0.7093067765235901\n",
      "Rank 15: Test MSE for model_Adam_MeanSquaredError_batch32.h5: 0.7734929323196411\n",
      "Rank 16: Test MSE for model_Adam_MeanSquaredError_batch64.h5: 0.8019177317619324\n"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    'model_Adam_MeanAbsoluteError_batch32.h5', 'model_Adam_MeanAbsoluteError_batch64.h5',\n",
    "    'model_Adam_MeanSquaredError_batch32.h5', 'model_Adam_MeanSquaredError_batch64.h5',\n",
    "    'model_Adam_Huber_batch32.h5', 'model_Adam_Huber_batch64.h5',\n",
    "    'model_Adam_LogCosh_batch32.h5', 'model_Adam_LogCosh_batch64.h5',\n",
    "    'model_RMSprop_MeanAbsoluteError_batch32.h5', 'model_RMSprop_MeanAbsoluteError_batch64.h5',\n",
    "    'model_RMSprop_MeanSquaredError_batch32.h5', 'model_RMSprop_MeanSquaredError_batch64.h5',\n",
    "    'model_RMSprop_Huber_batch32.h5', 'model_RMSprop_Huber_batch64.h5',\n",
    "    'model_RMSprop_LogCosh_batch32.h5', 'model_RMSprop_LogCosh_batch64.h5'\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_path in model_paths:\n",
    "    model = load_model(model_path)\n",
    "    evaluation = model.evaluate([test.user.values, test.book.values], test.rating.values)\n",
    "    results.append((model_path, evaluation))\n",
    "\n",
    "# Sort the results based on test MSE in ascending order\n",
    "results.sort(key=lambda x: x[1])\n",
    "\n",
    "# Print the models and their corresponding test MSE in ranking order\n",
    "for i, (model_path, evaluation) in enumerate(results, start=1):\n",
    "    print(f'Rank {i}: Test MSE for {model_path}: {evaluation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
