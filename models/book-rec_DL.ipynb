{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 12:35:21.260866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import MeanAbsoluteError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "We read the CSV file and load it into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('../Data/training/ratings.csv')\n",
    "\n",
    "books = pd.read_csv('../Data/training/books.csv')\n",
    "book_id_to_name = pd.Series(books.title.values, index = books.index).to_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first few records and a summary of the data for a quick examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book_id  user_id  rating\n",
      "0        1      314       5\n",
      "1        1      439       3\n",
      "2        1      588       5\n",
      "3        1     1169       4\n",
      "4        1     1185       4\n",
      "             book_id        user_id         rating\n",
      "count  981756.000000  981756.000000  981756.000000\n",
      "mean     4943.275636   25616.759933       3.856534\n",
      "std      2873.207415   15228.338826       0.983941\n",
      "min         1.000000       1.000000       1.000000\n",
      "25%      2457.000000   12372.000000       3.000000\n",
      "50%      4921.000000   25077.000000       4.000000\n",
      "75%      7414.000000   38572.000000       5.000000\n",
      "max     10000.000000   53424.000000       5.000000\n"
     ]
    }
   ],
   "source": [
    "print(ratings.head())\n",
    "print(ratings.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glkh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fewer than X</th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>17714</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>11305</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>5859</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>3907</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2759</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>2082</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>35</td>\n",
       "      <td>1671</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40</td>\n",
       "      <td>1305</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>45</td>\n",
       "      <td>1020</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>875</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55</td>\n",
       "      <td>693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>60</td>\n",
       "      <td>571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>65</td>\n",
       "      <td>462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>70</td>\n",
       "      <td>452</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75</td>\n",
       "      <td>337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>80</td>\n",
       "      <td>339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>85</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>95</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fewer than X  count  percent\n",
       "0              5  17714       33\n",
       "1             10  11305       21\n",
       "2             15   5859       11\n",
       "3             20   3907        7\n",
       "4             25   2759        5\n",
       "5             30   2082        4\n",
       "6             35   1671        3\n",
       "7             40   1305        2\n",
       "8             45   1020        2\n",
       "9             50    875        2\n",
       "10            55    693        1\n",
       "11            60    571        1\n",
       "12            65    462        1\n",
       "13            70    452        1\n",
       "14            75    337        1\n",
       "15            80    339        1\n",
       "16            85    266        0\n",
       "17            90    245        0\n",
       "18            95    177        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings = ratings.groupby('user_id')['rating'].count()\n",
    "user_rating_counts = ratings['user_id'].value_counts()\n",
    "# Count the number of users for each number of ratings\n",
    "users_with_ratings = user_rating_counts.groupby(user_ratings).count()\n",
    "# Create a list of rating count thresholds\n",
    "rating_thresholds = list(range(5, 100, 5))\n",
    "\n",
    "# Count the number of users with fewer than X ratings, excluding the previous ranks\n",
    "count_per_threshold = []\n",
    "previous_count = 0\n",
    "total_users = 53424  # Total number of users\n",
    "for threshold in rating_thresholds:\n",
    "    count = user_ratings[user_ratings < threshold].count() - previous_count\n",
    "    count_per_threshold.append(count)\n",
    "    previous_count += count\n",
    "\n",
    "# Calculate the percentage of the whole user base\n",
    "percent_per_threshold = [round((count / total_users) * 100) for count in count_per_threshold]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\"fewer than X\": rating_thresholds, \"count\": count_per_threshold, \"percent\": percent_per_threshold})\n",
    "\n",
    "# Print the DataFrame\n",
    "(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>new_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  rating_count  new_data\n",
       "0        0            76     False\n",
       "1        1            16     False\n",
       "2        2            24     False\n",
       "3        3            19     False\n",
       "4        4            42     False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_out= 10\n",
    "filtered_ratings = ratings[~ratings['user_id'].isin(user_rating_counts[user_rating_counts < filter_out].index.tolist())]\n",
    "filtered_ratings.loc[:, 'user_id'] = filtered_ratings.groupby('user_id').ngroup()\n",
    "# Get unique user IDs from the ratings data\n",
    "\n",
    "\n",
    "# Count the number of ratings per user\n",
    "rating_counts = filtered_ratings.groupby('user_id').size().reset_index(name='rating_count')\n",
    "\n",
    "# Create the users DataFrame\n",
    "users = pd.DataFrame(rating_counts)\n",
    "ratings = filtered_ratings\n",
    "users['new_data'] = False\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.to_csv('../Data/ratings.csv',index=False)\n",
    "users.to_csv('../data/users.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user-id and book-id mapping\n",
    "We're creating two mapping dictionaries for users and books - from id to index and from index to id.  \n",
    "This will help in embedding layer where we'll be dealing with indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = ratings['user_id'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "book_ids = ratings['book_id'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map user-id and book-ids to user and book indices\n",
    "We're creating two new columns in our DataFrame to hold the indices of users and books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings['user'] = ratings['user_id'].map(user2user_encoded)\n",
    "ratings['book'] = ratings['book_id'].map(book2book_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and testing set\n",
    "We split our data into a training set (80%) and a test set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of users and books\n",
    "We calculate the total number of unique users and books in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set embedding dimension\n",
    "This is a hyperparameter for our model representing the size of the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "We're using Keras Functional API to build a model with Embedding layers for users and books.  \n",
    "These embeddings will learn to represent user preferences and book properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=[1])\n",
    "user_embedding = Embedding(num_users, embedding_size)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "book_input = Input(shape=[1])\n",
    "book_embedding = Embedding(num_books, embedding_size)(book_input)\n",
    "book_vec = Flatten()(book_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the dot product of these vectors to predict the user's rating of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = Dot(axes=1)([book_vec, user_vec])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model takes as input the user and book indices, and outputs the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[user_input, book_input], outputs=product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10720/10720 [==============================] - 41s 4ms/step - loss: 3.4149 - val_loss: 2.2575\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=MeanAbsoluteError(), optimizer=Adam())\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=1, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values))\n",
    "model.save(\"../models/mae_best_model.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile our model with a mean squared error loss function, perfect for regression problem, and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the best model\n",
    "mae_checkpoint_path = '../Data/mae_best_model.h5'\n",
    "mse_checkpoint_path = '../Data/mse_best_model.h5'\n",
    "\n",
    "# Define a callback for model checkpointing\n",
    "mae_checkpoint = ModelCheckpoint(mae_checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "mse_checkpoint = ModelCheckpoint(mse_checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "mae_initial_weights=model.get_weights()\n",
    "mse_initial_weights=model.get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "We train our model for 5 epochs, with a batch size of 64. We also specify our validation data for validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MeanAbsoluteError(), optimizer=Adam())\n",
    "print('loss function=MeanAbsoluteError()')\n",
    "print('optimizer=Adam()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss=MeanAbsoluteError(), optimizer=RMSprop())\n",
    "print('optimizer=RMSprop()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss=MeanAbsoluteError(), optimizer=SGD())\n",
    "print('optimizer=SGD()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "print('loss function=mean_squared_error')\n",
    "print('optimizer=Adam()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop())\n",
    "print('optimizer=RMSprop()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD())\n",
    "print('optimizer=SGD()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "We evaluate our trained model on the test data to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('../book/mse_best_model.h5')\n",
    "mse = model.evaluate([test.user.values, test.book.values], test.rating.values)\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('../book/mae_best_model.h5')\n",
    "mae = model.evaluate([test.user.values, test.book.values], test.rating.values)\n",
    "print(f'Test MSE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each row in the DataFrame\n",
    "for index, row in books_df.iterrows():\n",
    "    image_url = row['image_url']\n",
    "    \n",
    "    \n",
    "    if image_url.startswith('https://s.gr-assets.com/'):\n",
    "        print(index,' startswith s.gr')\n",
    "        book_title = row['title']\n",
    "        search_title = transform_to_search_engine_friendly(book_title)\n",
    "        search_term = f\"{search_title}+book+cover+amazon\"\n",
    "        \n",
    "        # Construct the search URL\n",
    "        search_url = f\"https://www.googleapis.com/customsearch/v1?key={api_key}&cx={search_engine_id}&q={search_term}\"\n",
    "        print(search_url)\n",
    "        # Perform the search and retrieve the image URLs\n",
    "        response = requests.get(search_url)\n",
    "        search_results = response.json()\n",
    "        items = search_results.get(\"items\", [])  # Get the list of items from the search results\n",
    "\n",
    "        index = 0\n",
    "        image_url = None\n",
    "\n",
    "        while index < len(items) and image_url is None:\n",
    "            item = items[index]\n",
    "            pagemap = item.get(\"pagemap\", {})  # Get the pagemap dictionary of the item\n",
    "            scraped = pagemap.get(\"scraped\", [])  # Get the list of scraped items\n",
    "    \n",
    "            if scraped:\n",
    "                image_link = scraped[0].get(\"image_link\")  # Get the image link from the scraped item\n",
    "                if image_link:\n",
    "                    image_url = image_link  # Found an image link, assign it to image_url\n",
    "    \n",
    "        index += 1\n",
    "\n",
    "        print(image_url)\n",
    "\n",
    "        books_df.at[index, 'image_url'] = image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step\n",
      "[{'title': 'The Cabinet of Curiosities (Pendergast, #3)', 'image': 'https://images.gr-assets.com/books/1169235779m/39031.jpg', 'amazon_link': 'https://www.amazon.com/'}, {'title': 'Losing It (Losing It, #1)', 'image': 'https://images.gr-assets.com/books/1348459319m/16034964.jpg', 'amazon_link': 'https://www.amazon.com/'}, {'title': 'The Cat Who Could Read Backwards (Cat Who..., #1)', 'image': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png', 'amazon_link': 'https://www.amazon.com/'}, {'title': 'In Flight (Up in the Air, #1)', 'image': 'https://images.gr-assets.com/books/1397321579m/16134782.jpg', 'amazon_link': 'https://www.amazon.com/'}, {'title': \"Ender's Game, Volume 1: Battle School (Ender's Saga)\", 'image': 'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png', 'amazon_link': 'https://www.amazon.com/'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_id=123\n",
    "num_books=5\n",
    "book_id_to_name = pd.Series(books.title.values, index = books.index).to_dict()\n",
    "model = load_model('../models/mae_best_model.h5')\n",
    "ratings=pd.read_csv('../data/ratings.csv')\n",
    "books=pd.read_csv('../data/books.csv')\n",
    "user_ids = ratings['user_id'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "book_ids = ratings['book_id'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "user_encoded = user2user_encoded[user_id]\n",
    "# Getting the book ids in the encoding order\n",
    "book_ids = list(book2book_encoded.keys())\n",
    "book_ids = np.array(book_ids) - 1\n",
    "# Repeating the user id to match the shape of book ids\n",
    "user_array = np.array([user_encoded for _ in range(len(book_ids))])\n",
    "\n",
    "# Making the prediction\n",
    "pred_ratings = model.predict([user_array, np.array(book_ids)])\n",
    "\n",
    "# Getting the indices of the top num_books ratings\n",
    "top_indices = pred_ratings.flatten().argsort()[-num_books:][::-1]\n",
    "\n",
    " # Returning the corresponding book names\n",
    "recommended_books = []\n",
    "for i in top_indices:\n",
    "            book_id = book_ids[i] + 1\n",
    "            book_title = book_id_to_name[book_id]\n",
    "            book_image_url = books.loc[books['title'] == book_title, 'image_url'].values[0]\n",
    "            amazon_link = f\"https://www.amazon.com/\"\n",
    "            recommended_books.append({\n",
    "                \"title\": book_title,\n",
    "                \"image\": book_image_url,\n",
    "                \"amazon_link\": amazon_link\n",
    "            })\n",
    "print(recommended_books)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa\n"
     ]
    }
   ],
   "source": [
    "user_id = 1\n",
    "book_id = 1\n",
    "rating = 1\n",
    "users = pd.read_csv('../data/users.csv')\n",
    "if user_id in users['user_id'].values:\n",
    "        if user_id and rating:\n",
    "            print('lsa')\n",
    "\n",
    "        elif user_id:\n",
    "            print('didnt find any raiting for that user')\n",
    "            \n",
    "else:\n",
    "        print('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
