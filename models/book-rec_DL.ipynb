{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.losses import MeanAbsoluteError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "We read the CSV file and load it into a pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../book/ratings.csv')\n",
    "\n",
    "books_df = pd.read_csv('../book/books.csv')\n",
    "book_id_to_name = pd.Series(books_df.title.values, index = books_df.index).to_dict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the first few records and a summary of the data for a quick examination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   book_id  user_id  rating\n",
      "0        1      314       5\n",
      "1        1      439       3\n",
      "2        1      588       5\n",
      "3        1     1169       4\n",
      "4        1     1185       4\n",
      "             book_id        user_id         rating\n",
      "count  981756.000000  981756.000000  981756.000000\n",
      "mean     4943.275636   25616.759933       3.856534\n",
      "std      2873.207415   15228.338826       0.983941\n",
      "min         1.000000       1.000000       1.000000\n",
      "25%      2457.000000   12372.000000       3.000000\n",
      "50%      4921.000000   25077.000000       4.000000\n",
      "75%      7414.000000   38572.000000       5.000000\n",
      "max     10000.000000   53424.000000       5.000000\n"
     ]
    }
   ],
   "source": [
    "print(data.head())\n",
    "print(data.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_id    0\n",
      "user_id    0\n",
      "rating     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create user-id and book-id mapping\n",
    "We're creating two mapping dictionaries for users and books - from id to index and from index to id.  \n",
    "This will help in embedding layer where we'll be dealing with indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = data['user_id'].unique().tolist()\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "book_ids = data['book_id'].unique().tolist()\n",
    "book2book_encoded = {x: i for i, x in enumerate(book_ids)}\n",
    "book_encoded2book = {i: x for i, x in enumerate(book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save your dictionaries\n",
    "with open('../book/processed/user2user_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(user2user_encoded, f)\n",
    "\n",
    "with open('../book/processed/book2book_encoded.pkl', 'wb') as f:\n",
    "    pickle.dump(book2book_encoded, f)\n",
    "\n",
    "# save book_id to name mapping\n",
    "with open('../book/processed/book_id_to_name.pkl', 'wb') as f:\n",
    "    pickle.dump(book_id_to_name, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map user-id and book-ids to user and book indices\n",
    "We're creating two new columns in our DataFrame to hold the indices of users and books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['user'] = data['user_id'].map(user2user_encoded)\n",
    "data['book'] = data['book_id'].map(book2book_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and testing set\n",
    "We split our data into a training set (80%) and a test set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the number of users and books\n",
    "We calculate the total number of unique users and books in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = len(user2user_encoded)\n",
    "num_books = len(book_encoded2book)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set embedding dimension\n",
    "This is a hyperparameter for our model representing the size of the embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size=10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "We're using Keras Functional API to build a model with Embedding layers for users and books.  \n",
    "These embeddings will learn to represent user preferences and book properties during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=[1])\n",
    "user_embedding = Embedding(num_users, embedding_size)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "book_input = Input(shape=[1])\n",
    "book_embedding = Embedding(num_books, embedding_size)(book_input)\n",
    "book_vec = Flatten()(book_embedding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the dot product of these vectors to predict the user's rating of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = Dot(axes=1)([book_vec, user_vec])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model takes as input the user and book indices, and outputs the predicted rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[user_input, book_input], outputs=product)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile our model with a mean squared error loss function, perfect for regression problem, and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the best model\n",
    "mae_checkpoint_path = '../book/mae_best_model.h5'\n",
    "mse_checkpoint_path = '../book/mse_best_model.h5'\n",
    "\n",
    "# Define a callback for model checkpointing\n",
    "mae_checkpoint = ModelCheckpoint(mae_checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "mse_checkpoint = ModelCheckpoint(mse_checkpoint_path, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "mae_initial_weights=model.get_weights()\n",
    "mse_initial_weights=model.get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "We train our model for 5 epochs, with a batch size of 64. We also specify our validation data for validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=MeanAbsoluteError(), optimizer=Adam())\n",
    "print('loss function=MeanAbsoluteError()')\n",
    "print('optimizer=Adam()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss=MeanAbsoluteError(), optimizer=RMSprop())\n",
    "print('optimizer=RMSprop()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss=MeanAbsoluteError(), optimizer=SGD())\n",
    "print('optimizer=SGD()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mae_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mae_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer=Adam())\n",
    "print('loss function=mean_squared_error')\n",
    "print('optimizer=Adam()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=20, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss='mean_squared_error', optimizer=RMSprop())\n",
    "print('optimizer=RMSprop()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD())\n",
    "print('optimizer=SGD()')\n",
    "print('batch_size=8')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=8, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=16')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=16, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=32')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=32, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=64')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=64, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('batch_size=128')\n",
    "model.set_weights(mse_initial_weights)\n",
    "history = model.fit(x=[train.user.values, train.book.values], y=train.rating.values,\n",
    "                    batch_size=128, epochs=100, verbose=1,\n",
    "                    validation_data=([test.user.values, test.book.values], test.rating.values),\n",
    "                    callbacks=[mse_checkpoint])\n",
    "print('')\n",
    "print('')\n",
    "print('')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "We evaluate our trained model on the test data to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136/6136 [==============================] - 4s 584us/step - loss: 0.9376\n",
      "Test MSE: 0.9375975728034973\n"
     ]
    }
   ],
   "source": [
    "model=load_model('../book/mse_best_model.h5')\n",
    "mse = model.evaluate([test.user.values, test.book.values], test.rating.values)\n",
    "print(f'Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136/6136 [==============================] - 3s 545us/step - loss: 0.7050\n",
      "Test MSE: 0.7050154209136963\n"
     ]
    }
   ],
   "source": [
    "model=load_model('../book/mae_best_model.h5')\n",
    "mae = model.evaluate([test.user.values, test.book.values], test.rating.values)\n",
    "print(f'Test MSE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m book_id_to_name_keys \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(book_id_to_name\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m      4\u001b[0m \u001b[39m# Now find the difference\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m missing_ids \u001b[39m=\u001b[39m book_ids \u001b[39m-\u001b[39;49m book_id_to_name_keys\n\u001b[0;32m      7\u001b[0m \u001b[39mif\u001b[39;00m missing_ids:\n\u001b[0;32m      8\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe following book IDs are not included in the dictionary:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'set'"
     ]
    }
   ],
   "source": [
    "book_ids = data['book_id'].unique().tolist()\n",
    "book_id_to_name_keys = set(book_id_to_name.keys())\n",
    "\n",
    "# Now find the difference\n",
    "missing_ids = book_ids - book_id_to_name_keys\n",
    "\n",
    "if missing_ids:\n",
    "    print(\"The following book IDs are not included in the dictionary:\")\n",
    "    for book_id in missing_ids:\n",
    "        print(book_id)\n",
    "else:\n",
    "    print(\"All book IDs are included in the dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
